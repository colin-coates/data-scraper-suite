#!/usr/bin/env python3
"""
Test Scraper Engine

Tests the centralized orchestration engine, job dispatching, and plugin architecture.
"""

import asyncio
import sys
import os
from unittest.mock import Mock, patch

# Add the scraper suite to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from scraper_engine import ScraperEngine, EngineConfig
from core.base_scraper import ScraperConfig
from scrapers.linkedin_scraper import LinkedInScraper, LinkedInConfig
from scrapers.web_scraper import WebScraper, WebScraperConfig


async def test_scraper_engine():
    """Test the scraper engine functionality."""
    print("ğŸ•·ï¸ Testing Scraper Engine")
    print("=" * 40)

    # Test 1: Engine Initialization
    print("\n1ï¸âƒ£ Testing Engine Initialization:")
    try:
        config = EngineConfig(
            max_concurrent_jobs=2,
            enable_metrics=True,
            enable_anti_detection=False  # Disable for testing
        )
        engine = ScraperEngine(config)

        assert engine.config.max_concurrent_jobs == 2
        assert engine.config.enable_metrics == True
        assert len(engine.scrapers) == 0  # No scrapers registered yet

        print("âœ… Engine initialization successful")

    except Exception as e:
        print(f"âŒ Engine initialization failed: {e}")
        return False

    # Test 2: Scraper Registration
    print("\n2ï¸âƒ£ Testing Scraper Registration:")
    try:
        # Register LinkedIn scraper
        linkedin_config = LinkedInConfig(name="linkedin_test")
        engine.register_scraper("linkedin", LinkedInScraper, linkedin_config)

        # Register web scraper
        web_config = WebScraperConfig(name="web_test")
        engine.register_scraper("web", WebScraper, web_config)

        assert "linkedin" in engine.scrapers
        assert "web" in engine.scrapers
        assert len(engine.active_scrapers) == 2

        print("âœ… Scraper registration successful")
        print(f"   Registered scrapers: {list(engine.scrapers.keys())}")

    except Exception as e:
        print(f"âŒ Scraper registration failed: {e}")
        return False

    # Test 3: Job Dispatching
    print("\n3ï¸âƒ£ Testing Job Dispatching:")
    try:
        # Mock the initialization to avoid Azure dependencies
        with patch.object(engine, 'initialize', return_value=None):
            await engine.initialize()

        # Dispatch LinkedIn job
        linkedin_job = {
            "scraper_type": "linkedin",
            "target": {
                "profile_url": "https://linkedin.com/in/john-doe"
            },
            "priority": "high"
        }
        job_id_1 = await engine.dispatch_job(linkedin_job)

        # Dispatch web scraping job
        web_job = {
            "scraper_type": "web",
            "target": {
                "url": "https://example.com"
            },
            "priority": "normal"
        }
        job_id_2 = await engine.dispatch_job(web_job)

        assert job_id_1.startswith("job_")
        assert job_id_2.startswith("job_")
        assert job_id_1 != job_id_2

        print("âœ… Job dispatching successful")
        print(f"   Job 1 ID: {job_id_1}")
        print(f"   Job 2 ID: {job_id_2}")

    except Exception as e:
        print(f"âŒ Job dispatching failed: {e}")
        return False

    # Test 4: Job Status Tracking
    print("\n4ï¸âƒ£ Testing Job Status Tracking:")
    try:
        status_1 = engine.get_job_status(job_id_1)
        status_2 = engine.get_job_status(job_id_2)

        assert status_1 is not None
        assert status_2 is not None
        assert status_1['status'] == 'pending'
        assert status_2['status'] == 'pending'

        print("âœ… Job status tracking works")
        print(f"   Job 1 status: {status_1['status']}")
        print(f"   Job 2 status: {status_2['status']}")

    except Exception as e:
        print(f"âŒ Job status tracking failed: {e}")
        return False

    # Test 5: Engine Metrics
    print("\n5ï¸âƒ£ Testing Engine Metrics:")
    try:
        metrics = engine.get_metrics()

        assert 'jobs_dispatched' in metrics
        assert 'jobs_completed' in metrics
        assert 'success_rate' in metrics
        assert metrics['jobs_dispatched'] == 2
        assert 'registered_scrapers' in metrics
        assert len(metrics['registered_scrapers']) == 2

        print("âœ… Engine metrics collection works")
        print(f"   Jobs dispatched: {metrics['jobs_dispatched']}")
        print(f"   Success rate: {metrics['success_rate']:.2f}")
        print(f"   Registered scrapers: {metrics['registered_scrapers']}")

    except Exception as e:
        print(f"âŒ Engine metrics failed: {e}")
        return False

    # Test 6: Scraper Metrics
    print("\n6ï¸âƒ£ Testing Scraper Metrics:")
    try:
        linkedin_metrics = engine.get_scraper_metrics("linkedin")
        web_metrics = engine.get_scraper_metrics("web")

        assert linkedin_metrics is not None
        assert web_metrics is not None
        assert 'success_count' in linkedin_metrics
        assert 'error_count' in linkedin_metrics

        print("âœ… Scraper metrics collection works")
        print(f"   LinkedIn scraper metrics: {linkedin_metrics['success_count']} success, {linkedin_metrics['error_count']} errors")
        print(f"   Web scraper metrics: {web_metrics['success_count']} success, {web_metrics['error_count']} errors")

    except Exception as e:
        print(f"âŒ Scraper metrics failed: {e}")
        return False

    # Test 7: Error Handling
    print("\n7ï¸âƒ£ Testing Error Handling:")
    try:
        # Try to dispatch job with invalid scraper
        invalid_job = {
            "scraper_type": "nonexistent",
            "target": {"url": "https://example.com"}
        }

        try:
            await engine.dispatch_job(invalid_job)
            assert False, "Should have raised ValueError"
        except ValueError as e:
            assert "nonexistent" in str(e)

        print("âœ… Error handling works correctly")

    except Exception as e:
        print(f"âŒ Error handling test failed: {e}")
        return False

    # Cleanup
    try:
        await engine.cleanup()
        print("âœ… Engine cleanup successful")
    except Exception as e:
        print(f"âš ï¸ Engine cleanup warning: {e}")

    print("\nğŸ•·ï¸ Scraper Engine testing complete!")
    print("The scraper engine successfully manages job dispatching and scraper orchestration!")
    return True


async def test_plugin_architecture():
    """Test the plugin architecture components."""
    print("\nğŸ”Œ Testing Plugin Architecture")
    print("=" * 40)

    # Test 1: Plugin Manager Import
    print("\n1ï¸âƒ£ Testing Plugin Manager:")
    try:
        from core.plugin_manager import PluginManager

        pm = PluginManager()
        assert pm is not None

        print("âœ… Plugin manager import successful")

    except Exception as e:
        print(f"âŒ Plugin manager import failed: {e}")
        return False

    # Test 2: Plugin Discovery
    print("\n2ï¸âƒ£ Testing Plugin Discovery:")
    try:
        discovered = pm.discover_plugins()
        print(f"   Discovered plugins: {discovered}")

        # Should find our test plugins
        assert len(discovered) >= 2  # linkedin_scraper and web_scraper

        print("âœ… Plugin discovery works")

    except Exception as e:
        print(f"âŒ Plugin discovery failed: {e}")
        return False

    # Test 3: Plugin Loading
    print("\n3ï¸âƒ£ Testing Plugin Loading:")
    try:
        # Try to load linkedin scraper
        success = pm.load_plugin("scrapers.linkedin_scraper")
        if success:
            print("âœ… LinkedIn scraper plugin loaded")
        else:
            print("âš ï¸ LinkedIn scraper plugin failed to load (expected in test env)")

        # Try to load web scraper
        success = pm.load_plugin("scrapers.web_scraper")
        if success:
            print("âœ… Web scraper plugin loaded")
        else:
            print("âš ï¸ Web scraper plugin failed to load (expected in test env)")

        print("âœ… Plugin loading test completed")

    except Exception as e:
        print(f"âŒ Plugin loading failed: {e}")
        return False

    # Cleanup
    try:
        await pm.cleanup()
        print("âœ… Plugin manager cleanup successful")
    except Exception as e:
        print(f"âš ï¸ Plugin manager cleanup warning: {e}")

    return True


if __name__ == "__main__":
    async def main():
        try:
            # Test scraper engine
            engine_success = await test_scraper_engine()

            # Test plugin architecture
            plugin_success = await test_plugin_architecture()

            if engine_success and plugin_success:
                print("\nğŸ‰ All tests passed!")
                sys.exit(0)
            else:
                print("\nğŸ’¥ Some tests failed!")
                sys.exit(1)

        except Exception as e:
            print(f"\nğŸ’¥ Test suite failed with exception: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)

    asyncio.run(main())
