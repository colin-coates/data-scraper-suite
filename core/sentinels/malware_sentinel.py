# Copyright (c) 2024 Mountain Jewels Intelligence. All rights reserved.
#
# This software is proprietary and confidential. Unauthorized copying,
# modification, distribution, or use is strictly prohibited.

"""
Malware Sentinel for MJ Data Scraper Suite

Detects malware, phishing attempts, malicious content, and security threats
on target websites to protect scraping operations and infrastructure.
"""

import asyncio
import logging
import re
import hashlib
from typing import Dict, Any, List, Optional, Tuple, Set
from datetime import datetime
from urllib.parse import urlparse, urljoin

try:
    import aiohttp
    AIOHTTP_AVAILABLE = True
except ImportError:
    AIOHTTP_AVAILABLE = False

from .base import BaseSentinel, SentinelReport

logger = logging.getLogger(__name__)


class MalwareSentinel(BaseSentinel):
    """
    Monitors websites for malware, phishing, and security threats.

    Analyzes HTTP responses, content, scripts, and network behavior to detect
    malicious activities that could compromise scraping operations or infrastructure.
    """

    name = "malware_sentinel"

    def __init__(self):
        # Malware signatures and patterns
        self.malware_signatures = {
            "crypto_mining": {
                "content": ["coinhive", "cryptoloot", "webmine", "miner.start",
                           "coin-hive", "jsecoin", "minero"],
                "scripts": ["coinhive.min.js", "cryptoloot.min.js", "webmine.js"],
                "risk_weight": 4
            },
            "malicious_scripts": {
                "content": ["eval(", "document.write(", "innerHTML", "outerHTML"],
                "scripts": [".php?", ".exe", ".bat", ".scr"],
                "obfuscated": ["fromCharCode", "unescape(", "decodeURIComponent"],
                "risk_weight": 3
            },
            "drive_by_download": {
                "content": ["download.php", "setup.exe", "install.exe", "update.exe"],
                "headers": ["content-disposition: attachment"],
                "risk_weight": 5
            },
            "phishing_indicators": {
                "content": ["login", "password", "signin", "verify", "secure",
                           "bank", "paypal", "amazon", "facebook", "google"],
                "forms": ["password", "credit_card", "ssn", "social_security"],
                "risk_weight": 4
            },
            "malicious_redirects": {
                "content": ["window.location", "document.location", "location.href"],
                "meta_refresh": ["http-equiv=\"refresh\""],
                "risk_weight": 3
            },
            "suspicious_iframes": {
                "content": ["<iframe", "</iframe>"],
                "hidden_iframes": ["display:none", "visibility:hidden"],
                "risk_weight": 2
            },
            "exploit_kits": {
                "content": ["exploit", "vulnerability", "zero-day", "rce"],
                "scripts": ["exploit.js", "payload.js", "shell.php"],
                "risk_weight": 5
            },
            "adware_malware": {
                "content": ["adware", "malware", "trojan", "virus", "ransomware"],
                "scripts": ["ad.js", "track.js", "analytics.js"],
                "risk_weight": 3
            }
        }

        # Known malicious domains/IPs (sample - would be updated regularly)
        self.known_malicious_domains = {
            "malicious.example.com",
            "phishing.fakebank.com",
            "malware.downloads.ru",
            # Add real known malicious domains here
        }

        # Suspicious TLD patterns
        self.suspicious_tlds = {
            ".tk", ".ml", ".ga", ".cf", ".gq", ".xyz", ".top", ".club", ".online"
        }

        # Risk thresholds
        self.high_risk_threshold = 6
        self.critical_risk_threshold = 10

        # Content analysis limits
        self.max_content_length = 1024 * 1024  # 1MB limit for analysis
        self.suspicious_keywords_threshold = 3

        # Historical tracking
        self.threat_history = {}
        self.history_window = 10

    async def probe(self, target: Dict[str, Any]) -> SentinelReport:
        """
        Probe target websites for malware and security threats.

        Args:
            target: Target information containing URLs to analyze

        Returns:
            SentinelReport with threat assessment and recommendations
        """
        try:
            urls = self._extract_urls(target)

            if not urls:
                return SentinelReport(
                    sentinel_name=self.name,
                    domain="malware",
                    timestamp=datetime.utcnow(),
                    risk_level="low",
                    findings={"status": "no_urls_provided"},
                    recommended_action="allow"
                )

            # Analyze each URL for threats
            threat_analysis = []
            total_risk_score = 0

            for url in urls[:3]:  # Limit to 3 URLs for performance
                analysis = await self._analyze_url_threats(url)
                threat_analysis.append(analysis)
                total_risk_score += analysis.get("risk_score", 0)

            # Aggregate findings
            all_findings = self._aggregate_threats(threat_analysis)

            # Track threat history
            for url in urls:
                domain = self._extract_domain(url)
                if domain not in self.threat_history:
                    self.threat_history[domain] = []
                self.threat_history[domain].append({
                    "timestamp": datetime.utcnow(),
                    "risk_score": total_risk_score,
                    "threats": all_findings
                })

                # Keep only recent history
                if len(self.threat_history[domain]) > self.history_window:
                    self.threat_history[domain] = self.threat_history[domain][-self.history_window:]

            findings = {
                "urls_analyzed": len(urls),
                "total_risk_score": total_risk_score,
                "threat_analysis": threat_analysis,
                "detected_threats": all_findings,
                "threat_history": self.threat_history
            }

            # Determine risk level and recommended action
            if total_risk_score >= self.critical_risk_threshold:
                risk_level = "critical"
                recommended_action = "block"
            elif total_risk_score >= self.high_risk_threshold:
                risk_level = "high"
                recommended_action = "restrict"
            elif total_risk_score >= 3:
                risk_level = "medium"
                recommended_action = "delay"
            else:
                risk_level = "low"
                recommended_action = "allow"

            return SentinelReport(
                sentinel_name=self.name,
                domain="malware",
                timestamp=datetime.utcnow(),
                risk_level=risk_level,
                findings=findings,
                recommended_action=recommended_action
            )

        except Exception as e:
            logger.error(f"Malware probe failed: {e}")
            return SentinelReport(
                sentinel_name=self.name,
                domain="malware",
                timestamp=datetime.utcnow(),
                risk_level="critical",
                findings={"error": str(e), "error_type": type(e).__name__},
                recommended_action="block"
            )

    def _extract_urls(self, target: Dict[str, Any]) -> List[str]:
        """Extract URLs from target information."""
        urls = []

        # Extract direct URLs
        direct_urls = target.get("urls", [])
        if isinstance(direct_urls, str):
            direct_urls = [direct_urls]
        urls.extend(direct_urls)

        # Extract from single URL field
        single_url = target.get("url")
        if single_url:
            urls.append(single_url)

        # Extract from scraping targets
        targets = target.get("targets", [])
        for t in targets:
            if isinstance(t, str) and (t.startswith("http://") or t.startswith("https://")):
                urls.append(t)
            elif isinstance(t, dict) and "url" in t:
                urls.append(t["url"])

        # Remove duplicates and validate URLs
        valid_urls = []
        seen = set()
        for url in urls:
            if url and url not in seen:
                if url.startswith(("http://", "https://")):
                    valid_urls.append(url)
                    seen.add(url)

        return valid_urls[:5]  # Limit to 5 URLs for performance

    def _extract_domain(self, url: str) -> str:
        """Extract domain from URL."""
        try:
            parsed = urlparse(url)
            return parsed.netloc
        except Exception:
            return url

    async def _analyze_url_threats(self, url: str) -> Dict[str, Any]:
        """Analyze a single URL for malware and security threats."""
        analysis = {
            "url": url,
            "domain": self._extract_domain(url),
            "threats_detected": [],
            "risk_score": 0,
            "response_info": {},
            "content_hash": None
        }

        try:
            if not AIOHTTP_AVAILABLE:
                analysis["error"] = "aiohttp not available"
                return analysis

            # Check domain reputation first
            domain = analysis["domain"]
            domain_risk = self._analyze_domain_reputation(domain)
            if domain_risk:
                analysis["threats_detected"].append(domain_risk)
                analysis["risk_score"] += domain_risk.get("risk_weight", 0)

            timeout = aiohttp.ClientTimeout(total=15, connect=10)
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }

            async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
                async with session.get(url) as response:
                    analysis["response_info"] = {
                        "status_code": response.status,
                        "headers": dict(response.headers),
                        "content_type": response.headers.get("content-type", ""),
                        "content_length": response.headers.get("content-length", 0)
                    }

                    # Analyze response headers for threats
                    headers_text = "\n".join(f"{k}: {v}" for k, v in response.headers.items()).lower()
                    header_threats = self._analyze_headers(headers_text)
                    analysis["threats_detected"].extend(header_threats)

                    # Get and analyze response content
                    try:
                        content_length = int(response.headers.get("content-length", 0))
                        if content_length > self.max_content_length:
                            analysis["threats_detected"].append({
                                "type": "large_content",
                                "description": f"Content too large: {content_length} bytes",
                                "risk_weight": 1
                            })
                            content = await response.text()  # Still get some content for analysis
                            content = content[:10000]  # Limit analysis
                        else:
                            content = await response.text()

                        # Generate content hash for tracking
                        analysis["content_hash"] = hashlib.md5(content.encode()).hexdigest()

                        # Analyze content for threats
                        content_threats = self._analyze_content(content, headers_text, response.status)
                        analysis["threats_detected"].extend(content_threats)

                    except Exception as e:
                        analysis["content_error"] = str(e)

        except asyncio.TimeoutError:
            analysis["error"] = "request_timeout"
            analysis["risk_score"] += 2
        except aiohttp.ClientError as e:
            analysis["error"] = f"client_error: {str(e)}"
            analysis["risk_score"] += 1
        except Exception as e:
            analysis["error"] = f"analysis_error: {str(e)}"
            analysis["risk_score"] += 1

        # Calculate total risk score
        analysis["risk_score"] += sum(t.get("risk_weight", 0) for t in analysis["threats_detected"])

        return analysis

    def _analyze_domain_reputation(self, domain: str) -> Optional[Dict[str, Any]]:
        """Analyze domain for reputation issues."""
        # Check known malicious domains
        if domain in self.known_malicious_domains:
            return {
                "type": "known_malicious_domain",
                "description": f"Domain {domain} is in known malicious list",
                "risk_weight": 5
            }

        # Check suspicious TLDs
        for tld in self.suspicious_tlds:
            if domain.endswith(tld):
                return {
                    "type": "suspicious_tld",
                    "description": f"Domain uses suspicious TLD: {tld}",
                    "risk_weight": 2
                }

        # Check for suspicious patterns
        if re.search(r'\d{4,}', domain):  # Long numbers in domain
            return {
                "type": "suspicious_domain_pattern",
                "description": "Domain contains suspicious numeric patterns",
                "risk_weight": 1
            }

        return None

    def _analyze_headers(self, headers_text: str) -> List[Dict[str, Any]]:
        """Analyze HTTP headers for security threats."""
        threats = []

        # Check for suspicious headers
        suspicious_headers = [
            ("x-powered-by", "Server information disclosure"),
            ("server", "Server version disclosure"),
            ("x-aspnet-version", ".NET version disclosure"),
            ("x-debug", "Debug information disclosure")
        ]

        for header_name, description in suspicious_headers:
            if header_name in headers_text:
                threats.append({
                    "type": "information_disclosure",
                    "description": f"{description} in headers",
                    "risk_weight": 1
                })

        # Check for drive-by download indicators
        if "content-disposition: attachment" in headers_text:
            threats.append({
                "type": "drive_by_download",
                "description": "Forced file download detected",
                "risk_weight": 4
            })

        return threats

    def _analyze_content(self, content: str, headers_text: str, status_code: int) -> List[Dict[str, Any]]:
        """Analyze content for malware and security threats."""
        threats = []
        content_lower = content.lower()

        # Check for malware signatures
        for malware_type, signatures in self.malware_signatures.items():
            matches = []

            # Check content patterns
            for pattern in signatures.get("content", []):
                if pattern.lower() in content_lower:
                    matches.append(f"content:{pattern}")

            # Check script patterns
            for script in signatures.get("scripts", []):
                if script.lower() in content_lower:
                    matches.append(f"script:{script}")

            # Check obfuscated code patterns
            for obfuscated in signatures.get("obfuscated", []):
                if obfuscated.lower() in content_lower:
                    matches.append(f"obfuscated:{obfuscated}")

            # Check form patterns
            for form_pattern in signatures.get("forms", []):
                if form_pattern.lower() in content_lower:
                    matches.append(f"form:{form_pattern}")

            if matches:
                threats.append({
                    "type": malware_type,
                    "matches": matches,
                    "risk_weight": signatures["risk_weight"]
                })

        # Check for excessive suspicious keywords
        suspicious_keywords = [
            "password", "credit", "card", "ssn", "social", "security",
            "bank", "paypal", "bitcoin", "crypto", "wallet", "private",
            "key", "login", "signin", "verify", "secure", "account"
        ]

        keyword_count = sum(1 for keyword in suspicious_keywords if keyword in content_lower)
        if keyword_count >= self.suspicious_keywords_threshold:
            threats.append({
                "type": "suspicious_content",
                "description": f"High concentration of suspicious keywords: {keyword_count}",
                "risk_weight": min(keyword_count // 2, 3)
            })

        # Check for hidden elements (potential phishing)
        hidden_patterns = ["display:none", "visibility:hidden", "opacity:0"]
        hidden_count = sum(1 for pattern in hidden_patterns if pattern in content_lower)
        if hidden_count >= 2:
            threats.append({
                "type": "hidden_elements",
                "description": f"Multiple hidden elements detected: {hidden_count}",
                "risk_weight": 2
            })

        # Check for external script loading from suspicious domains
        script_domains = re.findall(r'<script[^>]*src=["\']([^"\']+)["\'][^>]*>', content, re.IGNORECASE)
        for script_url in script_domains:
            try:
                script_domain = self._extract_domain(script_url)
                if script_domain != self._extract_domain("") and script_domain in self.known_malicious_domains:
                    threats.append({
                        "type": "malicious_external_script",
                        "description": f"Script loaded from known malicious domain: {script_domain}",
                        "risk_weight": 4
                    })
            except:
                continue

        return threats

    def _aggregate_threats(self, analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Aggregate threat findings across multiple URL analyses."""
        aggregated = {
            "threat_types": set(),
            "total_risk_score": 0,
            "urls_with_threats": 0,
            "high_severity_threats": 0
        }

        for analysis in analyses:
            if analysis.get("risk_score", 0) > 0:
                aggregated["urls_with_threats"] += 1

            aggregated["total_risk_score"] += analysis.get("risk_score", 0)

            # Aggregate threat types
            for threat in analysis.get("threats_detected", []):
                aggregated["threat_types"].add(threat["type"])
                if threat.get("risk_weight", 0) >= 4:
                    aggregated["high_severity_threats"] += 1

        # Convert sets to lists for JSON serialization
        aggregated["threat_types"] = list(aggregated["threat_types"])

        return aggregated

    def update_risk_thresholds(self, high_threshold: int = None, critical_threshold: int = None) -> None:
        """Update threat assessment thresholds."""
        if high_threshold is not None:
            self.high_risk_threshold = high_threshold
        if critical_threshold is not None:
            self.critical_risk_threshold = critical_threshold

        logger.info(f"Updated malware risk thresholds: high={self.high_risk_threshold}, critical={self.critical_risk_threshold}")

    def add_malicious_domain(self, domain: str) -> None:
        """Add a domain to the known malicious list."""
        self.known_malicious_domains.add(domain.lower())
        logger.info(f"Added malicious domain: {domain}")

    def remove_malicious_domain(self, domain: str) -> None:
        """Remove a domain from the known malicious list."""
        self.known_malicious_domains.discard(domain.lower())
        logger.info(f"Removed malicious domain: {domain}")

    def get_threat_history(self, domain: str = None) -> Dict[str, Any]:
        """Get threat detection history."""
        if domain:
            return {domain: self.threat_history.get(domain, [])}
        return self.threat_history.copy()

    def update_content_limits(self, max_length: int = None, keyword_threshold: int = None) -> None:
        """Update content analysis limits."""
        if max_length is not None:
            self.max_content_length = max_length
        if keyword_threshold is not None:
            self.suspicious_keywords_threshold = keyword_threshold

        logger.info(f"Updated content limits: max_length={self.max_content_length}, keyword_threshold={self.suspicious_keywords_threshold}")


# Factory function for easy instantiation
def create_malware_sentinel(high_risk_threshold: int = 6, critical_risk_threshold: int = 10) -> MalwareSentinel:
    """
    Create a malware sentinel with sensible defaults.

    Args:
        high_risk_threshold: Risk score threshold for high risk
        critical_risk_threshold: Risk score threshold for critical risk

    Returns:
        Configured MalwareSentinel instance
    """
    sentinel = MalwareSentinel()
    sentinel.update_risk_thresholds(
        high_threshold=high_risk_threshold,
        critical_threshold=critical_risk_threshold
    )

    return sentinel
